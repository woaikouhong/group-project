Group Project
A new line
Another line in 'cat branch'

Our Group is going to do CASE 1,CASE 2.CASE 3.

Case 1

What facilities and equipment will be required (hard disk space, backup server, central repository, off-site repository, etc.) 
What data management practices (backups, storage, access control,
archiving etc.) will be used


In this case, the data size is big and is keep renewing every day. According to the case, the first part of data is the ROMV data, which is collected monthly and converted to NetCDF formatted data for storage. Every day, there will be in total 800MB NetCDF data being produced, and the NetCDF data size so far has reached 500GB.  The Second part of the data is the field notes, the size of the data collect form filed notes is around 2GB so far. Third part is the csv files of complex stimulation models, which accumulated to 200GB now. 
Our group conducted an estimation of a three-year data storage.  First for the NetCDF data, yearly storage that required is 800MB per day * 365 days equal to 285GB. In 3 years, NetCDF data would at least reach 285GB* 3 years which is 855GB. Second for the File notes the estimation is going to be doubled which is at lease 4GB in 3 years, due to the increase in funding. Third, suppose the stimulation models is going to increase at the same speed as the NetCDF data, therefore in 1 year it will reach (200GB/500GB) * 285GB=114GB. In three years would reach 114GB *3= 342GB. Therefore, in total the storage that Dr Periwinkle need will be at least 1201GB.


Also due to the increase funding, Dr. Periwinkles lab is now collecting substantially more data than ever before. The size of the data files is going to keep expanding, therefore the estimate storage is going to be larger than what our team predicted above. The storage requirement is high, approximately over 2 TB. The current and historical data are stored in hard drives but as the data size keep increasing the hard drive is not a very good choose. Hard drives are expensive and have the risk to be lost to disaster and will create unnecessary difficulty to organize the data in practical  sequence. The data is going to be renewed and accessed very frequently, making the hard drive a very inconvenient option for quick data access and sharing. Therefore, our team suggest Dr. Periwinkle to move all the data to the cloud drive. Cloud allows the  data owners to set up access control and also can set up  access limit by IP address and other types of user authorizations. Dr. Periwinkle can set up constraints on who can edit the internal data and can provide view only version for student’s general use. For the CSV files which contains the situation of model, the Version control is necessary, cloud drive would allow Dr. Periwinkle’s research team member to track the changes and keep different versions of the model. Backups are also important; the hard drive is useful, and it should save the all the raw data periodically on a hard drive and an another backup cloud drive, to prevent the loss of data, unauthorized access and incorrect modify.

Case 2

What facilities and equipment will be required and What data management practices

In this case the most important experiment data is not going to increase, and the biggest concern is the confidentiality of the data. According to the case and information provide in the discussion, the textual/content analysis file and open data is around 24 GB so far and Dr. Green estimate the size of the data may tripled to 72GB in the future. second part of the data is the MP3 files of the interview records. According to AudioMountain.com, one-hour 128kps Mp3 is 57.6MB (Appendix B), therefore, 15 one-hour interviews would need 864 MB storages. The transcriptions would be all word files which is going to take very small amount of storages. Since the size of the experiment data is not going to increase significantly, in total the estimation of the storage that Dr. Green needs is going to be less than 100GB.

The data need to be stored and protected over a ten years duration. Multiple backups should be saved to ensure the data safety during a long duration. Also, since the primary concern in this case is the security and confidentiality of the data. Dr. Green refuse to share the data with anyone, therefore there is no external access control need to be considered. Therefore, our team suggests Dr. Green keep three copies to ensure the data security.  A local copy could be stored in the personal computer located on the institutions campus. Additionally, the hard drive that Dr. Green’s is current using to keeps his master copy should continue to be used.  Also, a safe cloud could be used for daily access by Dr. Green and where his grad student can upload data. Currently, Dr. Green stores the MP3 files in Dropbox, transcriptions in Google doc, and textual files in the Zotero database. Since the information in the Dropbox and Google doc is the same, therefore our team suggests the data should be moved together under a single cloud server that is supported by Dalhousie University, Sharepoint. Sharepoint is not only a more logical cloud database for Dr. Green to centralize his research, but is now stored in Canada. Dropbox and Google has storage and disaster locations in the U.S., which comply with different data privacy policies than storage facilities hosted Canada. Due to the sensitive nature of the data, having the confidential data in a location that complies with Canada data privacy is imperative. 



